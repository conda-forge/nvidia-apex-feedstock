{% set version = "24.04.01" %}

package:
  name: nvidia-apex
  version: {{ version }}

source:
  url: https://github.com/NVIDIA/apex/archive/refs/tags/{{ version }}.tar.gz
  sha256: 065bc5c0146ee579d5db2b38ca3949da4dc799b871961a2c9eb19e18892166ce

build:
  number: 3
  skip: true  # [not linux or cuda_compiler == "None"]
  string: cuda{{ cuda_compiler_version | replace('.', '') }}py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}
  script_env:
    - TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX      # [cuda_compiler_version == "11.2"]
    - TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9+PTX  # [cuda_compiler_version == "11.8"]
    - TORCH_CUDA_ARCH_LIST=5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9;9.0+PTX  # [(cuda_compiler_version or "").startswith("12")]
  script:
    - python -m pip install . -vv

requirements:
  build:
    - {{ stdlib("c") }}
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}
  host:
    - python
    - pip
    - setuptools
    - packaging
    - pytorch
    - pytorch =*=cuda*
  run:
    - python
    - cxxfilt
    - tqdm
    - numpy
    - pyyaml
    - pytest
    - packaging
  run_constrained:
    - pytorch =*=cuda*

test:
  imports:
    - apex
    - apex.amp
    - apex.parallel
    - apex.optimizers
    - apex.normalization.fused_layer_norm

about:
  home: "https://nvidia.github.io/apex/"
  license: BSD-3-Clause
  license_family: BSD
  license_file: LICENSE
  summary: "a Pytorch extension with NVIDIA-maintained utilities to streamline mixed precision and distributed training."
  doc_url: "https://nvidia.github.io/apex/"
  dev_url: "https://github.com/NVIDIA/apex"

extra:
  recipe-maintainers:
    - h-vetinari
    - oblute
    - benhuff
    - jakirkham
    - rluria14
